{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "k200-features-extraction-tf-training-images-PART0.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p9MU1FUv8Iq"
      },
      "source": [
        "# Note: restart runtime after this import before running the augmentations\n",
        "!pip install -U augly\n",
        "!sudo apt-get install python3-magic\n",
        "\n",
        "PART = 0 # 10 parts\n",
        "DEBUG = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tkw6qFQLxYQ"
      },
      "source": [
        "s = \"\"\"\n",
        "dataset | n | size | s3_uri \n",
        "---------------------------------------------------------------------------------------------------- \n",
        "query images \n",
        "| 50,000 | 7GB | s3://drivendata-competition-fb-isc-data/all/query_images/ reference images \n",
        "| 1,000,000 | 178GB | s3://drivendata-competition-fb-isc-data/all/reference_images/ training images \n",
        "| 1,000,000 | 175GB | s3://drivendata-competition-fb-isc-data/all/training_images/\n",
        "aws s3 cp s3://drivendata-competition-fb-isc-data/all/query_images/ ./ --recursive --exclude=\"\" --include=\"Q00\" --no-sign-request\n",
        "\n",
        "# https://www.kaggle.com/philculliton/landmark-retrieval-2020-shared-scoring-script?scriptVersionId=38364319\n",
        "# https://www.kaggle.com/camaskew/baseline-landmark-retrieval-model\n",
        "\"\"\"\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euaxKp_c4Zi5"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# make sure your kaggle api \"kaggle.json\" file in your drive\n",
        "!mkdir -p /root/.kaggle\n",
        "!echo '{\"username\":\"tarobxl\",\"key\":\"a4b1e63bbff7f44f713dce2525191ba1\"}' > /root/.kaggle/kaggle.json\n",
        "#! cp '/content/drive/My Drive/kaggle.json' /root/.kaggle # <---- path for kaggle.json file\n",
        "!chmod 400 /root/.kaggle/kaggle.json\n",
        "!cat /root/.kaggle/kaggle.json\n",
        "\n",
        "!pip uninstall -y kaggle >> quit\n",
        "!pip install --upgrade pip >> quit\n",
        "!pip install kaggle==1.5.6 >> quit\n",
        "!kaggle -v >> quit\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIq7hC85Ml05"
      },
      "source": [
        "url_suffix = \"camaskew/baseline-landmark-retrieval-model\"\n",
        "!kaggle datasets download $url_suffix -p /content/ --unzip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nJ25aFrOBDv"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install awscli\n",
        "clear_output()\n",
        "print(\"aws is ready!\")\n",
        "\n",
        "temp_dir = \"/content/data/data_origin\"\n",
        "!mkdir -p $temp_dir\n",
        "!rm -rf $temp_dir/*\n",
        "%cd $temp_dir\n",
        "\n",
        "driven_data_s3 = \"s3://drivendata-competition-fb-isc-data/all/training_images/\"\n",
        "\n",
        "# Q00002.jpg => first 1K\n",
        "# R000000.jpg => \n",
        "#!aws s3 cp $driven_data_s3 ./ --recursive --exclude=\"*\" --include=\"Q00*\" --no-sign-request\n",
        "#!aws s3 cp $driven_data_s3 ./ --recursive --exclude=\"*\" --include=\"R00000*\" --no-sign-request\n",
        "#            R000000.jpg\n",
        "part_ref = f\"T{PART}*\"\n",
        "if DEBUG:\n",
        "    part_ref = f\"T000{PART}*\"\n",
        "\n",
        "!aws s3 cp $driven_data_s3 ./ --recursive --exclude=\"*\" --include=$part_ref --no-sign-request\n",
        "\n",
        "clear_output()\n",
        "!ls -alh *.jpg | wc\n",
        "!ls -alh * | head -5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cRCocJTN2Me"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from IPython.display import clear_output\n",
        "\n",
        "REQUIRED_SIGNATURE = 'serving_default'\n",
        "REQUIRED_OUTPUT = 'global_descriptor'\n",
        "    \n",
        "def load(saved_model_proto_filename):\n",
        "    saved_model_path = Path(saved_model_proto_filename).parent        \n",
        "    print (saved_model_path, saved_model_proto_filename)\n",
        "\n",
        "    model = tf.saved_model.load(str(saved_model_path))\n",
        "\n",
        "    found_signatures = list(model.signatures.keys())\n",
        "\n",
        "    if REQUIRED_SIGNATURE not in found_signatures:\n",
        "        return None\n",
        "\n",
        "    outputs = model.signatures[REQUIRED_SIGNATURE].structured_outputs\n",
        "    if REQUIRED_OUTPUT not in outputs:\n",
        "        return None\n",
        "\n",
        "    embedding_fn = model.signatures[REQUIRED_SIGNATURE]\n",
        "\n",
        "    return model, embedding_fn\n",
        "    \n",
        "model, embedding_fn = load(\"./baseline_landmark_retrieval_model/saved_model.pb\")\n",
        "print(\"-\"*80)\n",
        "clear_output()\n",
        "model, embedding_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJiqqvVjw1wa"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import augly.image as imaugs\n",
        "import augly.utils as utils\n",
        "\n",
        "aug1_compose = imaugs.Compose(\n",
        "    [\n",
        "        imaugs.PerspectiveTransform(sigma=20),\n",
        "        imaugs.OverlayEmoji()\n",
        "    ]\n",
        ")\n",
        "\n",
        "aug2_compose = imaugs.Compose(\n",
        "    [\n",
        "        imaugs.Saturation(factor=2.0),\n",
        "        imaugs.OverlayOntoScreenshot(\n",
        "            template_filepath=os.path.join(\n",
        "                utils.SCREENSHOT_TEMPLATES_DIR, \"mobile.png\"\n",
        "            ),\n",
        "        ),\n",
        "        imaugs.Scale(factor=0.6),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def aug1_function(input_img):\n",
        "    return aug1_compose(input_img)\n",
        "\n",
        "def aug2_function(input_img):\n",
        "    return aug2_compose(input_img)\n",
        "\n",
        "if DEBUG:\n",
        "    %cd /content/\n",
        "    \n",
        "    temp_dir = \"/content/data/data_origin\"\n",
        "    image_path = f\"{temp_dir}/T000000.jpg\"\n",
        "\n",
        "    input_img = Image.open(image_path) # imaugs.scale(input_img_path, factor=0.2)\n",
        "    display(aug2_function(input_img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcBBmHMnOR2f"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def get_embeddings(image_root_dir: str):\n",
        "    def get_id(image_path: Path):\n",
        "        return str(image_path.name).split(\".\")[0]\n",
        "    \n",
        "    def get_embedding_single(image_path: Path) -> np.ndarray:\n",
        "\n",
        "        image_input = Image.open(str(image_path))\n",
        "        image_aug1 = aug1_function(image_input)\n",
        "        #image_aug2 = aug2_function(image_input)\n",
        "\n",
        "        def get_emb(img):\n",
        "            image_data = np.array(img.convert('RGB'))\n",
        "            image_tensor = tf.convert_to_tensor(image_data)\n",
        "            return embedding_fn(image_tensor)[REQUIRED_OUTPUT].numpy()\n",
        "\n",
        "        image_id = get_id(image_path)\n",
        "\n",
        "        return [image_id, get_emb(image_input), get_emb(image_aug1)]\n",
        "\n",
        "    image_paths = [p for p in Path(image_root_dir).rglob('*.jpg')]\n",
        "    print(len(image_paths))\n",
        "    \n",
        "    embeddings = [get_embedding_single(image_path) \n",
        "                  for i, image_path in tqdm(enumerate(image_paths))]\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "if True:\n",
        "    import datetime\n",
        "    print(\"{date:%Y%m%d-%H%M%S}\".format(date=datetime.datetime.now()))\n",
        "\n",
        "    %cd /content/\n",
        "    embeddings = get_embeddings(temp_dir)\n",
        "    print(len(embeddings))\n",
        "\n",
        "    import pickle\n",
        "    data_features = {\"embeddings\": embeddings}\n",
        "    file_to_store = open(\"data_features.pickle\", \"wb\")\n",
        "    pickle.dump(data_features, file_to_store)\n",
        "    file_to_store.close()\n",
        "\n",
        "    print(\"{date:%Y%m%d-%H%M%S}\".format(date=datetime.datetime.now()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2CMCCWW35e7"
      },
      "source": [
        "for e in embeddings[0]:\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSPp8uRxPfiv"
      },
      "source": [
        "import datetime\n",
        "now_str = \"{date:%Y%m%d-%H%M%S}\".format(date=datetime.datetime.now())\n",
        "\n",
        "title = f\"k200-tf-training-{PART}-{now_str}\"\n",
        "if DEBUG:\n",
        "    title = f\"k200-tf-training-debug-{PART}-{now_str}\"\n",
        "\n",
        "print(title)\n",
        "\n",
        "#------------------------------\n",
        "!mkdir -p dataset\n",
        "!rm -rf dataset/*\n",
        "!cp *.pickle dataset\n",
        "\n",
        "data = '''{\n",
        "  \"title\": \"__title__\",\n",
        "  \"id\": \"tarobxl/__title__\",\n",
        "  \"licenses\": [\n",
        "    {\n",
        "      \"name\": \"CC0-1.0\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "'''.replace(\"__title__\", title)\n",
        "text_file = open(\"dataset/dataset-metadata.json\", 'w+')\n",
        "n = text_file.write(data)\n",
        "text_file.close()\n",
        "\n",
        "#!kaggle datasets create -p \"dataset\"\n",
        "\n",
        "!kaggle datasets create -p \"dataset\" --dir-mode zip"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}