{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# make sure your kaggle api \"kaggle.json\" file in your drive\n",
    "!mkdir -p /root/.kaggle\n",
    "!echo '{\"username\":\"tarobxl\",\"key\":\"a4b1e63bbff7f44f713dce2525191ba1\"}' > /root/.kaggle/kaggle.json\n",
    "#! cp '/content/drive/My Drive/kaggle.json' /root/.kaggle # <---- path for kaggle.json file\n",
    "!chmod 400 /root/.kaggle/kaggle.json\n",
    "!cat /root/.kaggle/kaggle.json\n",
    "\n",
    "!pip uninstall -y kaggle >> quit\n",
    "!pip install --upgrade pip >> quit\n",
    "!pip install kaggle==1.5.6 >> quit\n",
    "!kaggle -v >> quit\n",
    "\n",
    "clear_output()"
   ],
   "outputs": [],
   "metadata": {
    "id": "yfkG3vsejvkQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 1096), started 0:02:14 ago. (Use '!kill 1096' to kill it.)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cadd6591c0f3d1cc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cadd6591c0f3d1cc\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "filenames = ['../input/k200-tf-0-20210727-122841/data_features.pickle',\n",
    " '../input/k200-tf-1-20210727-123059/data_features.pickle',\n",
    " '../input/k200-tf-2-20210727-144220/data_features.pickle',\n",
    " '../input/k200-tf-3-20210727-170129/data_features.pickle',\n",
    " '../input/k200-tf-4-20210727-200848/data_features.pickle',\n",
    " '../input/k200-tf-5-20210728-174514/data_features.pickle',\n",
    " '../input/k200-tf-6-20210728-114118/data_features.pickle',\n",
    " '../input/k200-tf-7-20210728-115026/data_features.pickle',\n",
    " '../input/k200-tf-8-20210728-113919/data_features.pickle',\n",
    " '../input/k200-tf-9-20210728-113846/data_features.pickle',\n",
    " '../input/k200-tf-query-images-20210728-154427/data_features.pickle']\n",
    "\n",
    "datasets = [f.split(\"/\")[2] for f in filenames]\n",
    "datasets"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['k200-tf-0-20210727-122841',\n",
       " 'k200-tf-1-20210727-123059',\n",
       " 'k200-tf-2-20210727-144220',\n",
       " 'k200-tf-3-20210727-170129',\n",
       " 'k200-tf-4-20210727-200848',\n",
       " 'k200-tf-5-20210728-174514',\n",
       " 'k200-tf-6-20210728-114118',\n",
       " 'k200-tf-7-20210728-115026',\n",
       " 'k200-tf-8-20210728-113919',\n",
       " 'k200-tf-9-20210728-113846',\n",
       " 'k200-tf-query-images-20210728-154427']"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 16
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqe_BUXOkE3u",
    "outputId": "0bc42d37-6c50-4627-82ea-ce2aeaf2c1a6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!mkdir -p features\n",
    "!rm -rf features/*\n",
    "!rm -rf *.pickle\n",
    "!rm -rf *.zip\n",
    "for dataset in datasets:\n",
    "    url_suffix = f\"tarobxl/{dataset}\"\n",
    "    !kaggle datasets download $url_suffix -p /content/ --unzip\n",
    "    !mkdir -p features/$dataset\n",
    "    !mv data_features.pickle features/$dataset/"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading k200-tf-0-20210727-122841.zip to /content\n",
      " 99% 722M/729M [00:04<00:00, 147MB/s]\n",
      "100% 729M/729M [00:04<00:00, 155MB/s]\n",
      "Downloading k200-tf-1-20210727-123059.zip to /content\n",
      "100% 727M/729M [00:04<00:00, 169MB/s]\n",
      "100% 729M/729M [00:04<00:00, 182MB/s]\n",
      "Downloading k200-tf-2-20210727-144220.zip to /content\n",
      "100% 729M/729M [00:04<00:00, 101MB/s] \n",
      "100% 729M/729M [00:04<00:00, 164MB/s]\n",
      "Downloading k200-tf-3-20210727-170129.zip to /content\n",
      " 99% 719M/729M [00:07<00:00, 129MB/s]\n",
      "100% 729M/729M [00:07<00:00, 100MB/s]\n",
      "Downloading k200-tf-4-20210727-200848.zip to /content\n",
      " 98% 714M/729M [00:04<00:00, 201MB/s]\n",
      "100% 729M/729M [00:04<00:00, 158MB/s]\n",
      "Downloading k200-tf-5-20210728-174514.zip to /content\n",
      "100% 728M/729M [00:06<00:00, 109MB/s] \n",
      "100% 729M/729M [00:06<00:00, 116MB/s]\n",
      "Downloading k200-tf-6-20210728-114118.zip to /content\n",
      " 99% 724M/729M [00:03<00:00, 176MB/s]\n",
      "100% 729M/729M [00:03<00:00, 199MB/s]\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUMFE2VDj_me",
    "outputId": "937a48b8-0536-4071-a192-c508b1953184"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "query_filename = datasets[-1]\n",
    "\n",
    "ids = []\n",
    "embeddings = []\n",
    "for dataset in tqdm(datasets):\n",
    "    filename = f\"features/{dataset}/data_features.pickle\"\n",
    "    if \"query\" in filename:\n",
    "        query_filename = filename\n",
    "        print(query_filename)\n",
    "        continue\n",
    "    file_to_read = open(filename, \"rb\")\n",
    "    loaded_features = pickle.load(file_to_read)\n",
    "    file_to_read.close()\n",
    "    \n",
    "    ids += loaded_features[\"ids\"]\n",
    "    embeddings += loaded_features[\"embeddings\"]\n",
    "    \n",
    "print(len(ids), len(embeddings))\n",
    "ids[0], embeddings[0]"
   ],
   "outputs": [],
   "metadata": {
    "id": "b3pomZhYnlmt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ids, embeddings = zip(*sorted(zip(ids, embeddings)))\n",
    "print(ids[95763], embeddings[95763])\n",
    "\n",
    "X = np.vstack(embeddings)\n",
    "X.shape"
   ],
   "outputs": [],
   "metadata": {
    "id": "BtaqtFzToBuD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%capture\n",
    "if False:\n",
    "    !pip install pca\n",
    "    from pca import pca\n",
    "    #nb_dim = 64 # max 256\n",
    "    #model = pca(n_components=nb_dim, normalize=True)\n",
    "    #results = model.fit_transform(X)"
   ],
   "outputs": [],
   "metadata": {
    "id": "xyz0ihbFoMpl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "ZR43Ymyhr8g8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if False:\n",
    "    from sklearn.decomposition import IncrementalPCA\n",
    "    transformer = IncrementalPCA(n_components=64, batch_size=200)\n",
    "    transformer.partial_fit(X[:1*1000, :])\n",
    "    X_transformed = transformer.fit_transform(X)\n",
    "    X_transformed.shape"
   ],
   "outputs": [],
   "metadata": {
    "id": "DJJqUw7ipWoM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "n_comp = 64\n",
    "svd = decomposition.TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd.fit(X[:100*1000, :])\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "\n",
    "X_transformed = svd.transform(X)\n",
    "X_transformed.shape"
   ],
   "outputs": [],
   "metadata": {
    "id": "XvkM38cn7C3W"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "file_to_read = open(query_filename, \"rb\")\n",
    "loaded_features = pickle.load(file_to_read)\n",
    "file_to_read.close()\n",
    "\n",
    "query_ids = loaded_features[\"ids\"]\n",
    "query_embeddings = loaded_features[\"embeddings\"]\n",
    "\n",
    "print(query_ids[0], query_embeddings[0])\n",
    "\n",
    "query_ids, query_embeddings = zip(*sorted(zip(query_ids, query_embeddings)))\n",
    "print(query_ids[38391], query_embeddings[38391])\n",
    "\n",
    "query_X = np.vstack(query_embeddings)\n",
    "query_X.shape"
   ],
   "outputs": [],
   "metadata": {
    "id": "HEqETM8b9MLw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "query_X_transformed = svd.transform(query_X)\n",
    "query_X_transformed.shape"
   ],
   "outputs": [],
   "metadata": {
    "id": "EfvLysJx9cNZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "M_ref = X_transformed #np.random.rand(1_000_000, 256).astype('float32')\n",
    "M_query = query_X_transformed #np.random.rand(50_000, 256).astype('float32')\n",
    "print(M_ref.shape, M_query.shape)\n",
    "\n",
    "qry_ids = ['Q' + str(x).zfill(5) for x in range(50_000)]\n",
    "ref_ids = ['R' + str(x).zfill(6) for x in range(1_000_000)]\n",
    "\n",
    "out = \"fb-isc-submission.h5\"\n",
    "with h5py.File(out, \"w\") as f:\n",
    "    f.create_dataset(\"query\", data=M_query)\n",
    "    f.create_dataset(\"reference\", data=M_ref)\n",
    "    f.create_dataset('query_ids', data=qry_ids)\n",
    "    f.create_dataset('reference_ids', data=ref_ids)"
   ],
   "outputs": [],
   "metadata": {
    "id": "AXbcV8D49XqZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!ls -alh"
   ],
   "outputs": [],
   "metadata": {
    "id": "2Mv_bXNB-rJW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!curl --upload-file ./fb-isc-submission.h5 https://transfer.sh/fb-isc-submission.h5"
   ],
   "outputs": [],
   "metadata": {
    "id": "VzyL6DOY-4hN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from google.colab import files\n",
    "files.download('fb-isc-submission.h5')"
   ],
   "outputs": [],
   "metadata": {
    "id": "ygmFpnLt_M9E"
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "k200-pca.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('torch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "842ffcd55d1bdb8642c084c1bd6c6e9ed8db06ca4b283e41d24eeabb9f48db67"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}